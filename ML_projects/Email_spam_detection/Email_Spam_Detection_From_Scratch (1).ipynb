{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d69CcwmyMCu",
        "outputId": "5b0aecdf-9c8c-4ede-9624-1a658bef1520"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('emails.csv')\n",
        "\n",
        "# Display dataset info\n",
        "print(df.head(), '\\n')\n",
        "print(\"Shape of the dataframe:\", df.shape, '\\n')\n",
        "print(\"Columns of the dataframe:\", df.columns, '\\n')\n",
        "print(\"Summary of the data:\")\n",
        "df.info()\n",
        "\n",
        "# Ensure text is string and clean\n",
        "df['text'] = df['text'].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Remove stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Tokenization\n",
        "df['tokenized_text'] = df['text'].apply(word_tokenize)\n",
        "\n",
        "# POS tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "df['pos_tagged'] = df['tokenized_text'].apply(nltk.pos_tag)\n",
        "\n",
        "# Function to map POS tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized_text'] = df['pos_tagged'].apply(\n",
        "    lambda tagged_words: [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_words]\n",
        ")\n",
        "\n",
        "# Convert list of words into a single string per document\n",
        "df['processed_text'] = df['lemmatized_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n",
        "print(\"Selected features:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Extract labels\n",
        "y = df['spam']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning for Logistic Regression and Random Forest\n",
        "param_grid = {\n",
        "    \"Logistic Regression\": {'C': [0.1, 1, 10]},\n",
        "    \"Random Forest\": {'n_estimators': [50, 100, 200]}\n",
        "}\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    if model_name in param_grid:\n",
        "        grid_search = GridSearchCV(model, param_grid[model_name], cv=3, scoring='accuracy')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[model_name] = grid_search.best_estimator_\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_models[model_name] = model\n",
        "\n",
        "# Evaluate models\n",
        "for model_name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAaenTOkyuOH",
        "outputId": "d76d92dd-c4a1-42b9-a830-a25649c2a576"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  spam\n",
            "0  Subject: naturally irresistible your corporate...     1\n",
            "1  Subject: the stock trading gunslinger  fanny i...     1\n",
            "2  Subject: unbelievable new homes made easy  im ...     1\n",
            "3  Subject: 4 color printing special  request add...     1\n",
            "4  Subject: do not have money , get software cds ...     1 \n",
            "\n",
            "Shape of the dataframe: (5728, 2) \n",
            "\n",
            "Columns of the dataframe: Index(['text', 'spam'], dtype='object') \n",
            "\n",
            "Summary of the data:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5728 entries, 0 to 5727\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    5728 non-null   object\n",
            " 1   spam    5728 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 89.6+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['2000' 'cc' 'com' 'ect' 'enron' 'hou' 'kaminski' 'pm' 'subject' 'vince']\n",
            "Training Naive Bayes...\n",
            "Training Logistic Regression...\n",
            "Training SVM...\n",
            "Training Random Forest...\n",
            "Naive Bayes Accuracy: 0.8944\n",
            "Logistic Regression Accuracy: 0.8953\n",
            "SVM Accuracy: 0.8944\n",
            "Random Forest Accuracy: 0.8927\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}